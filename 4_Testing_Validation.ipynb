{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸš‘ AmbuRoute - Testing & Validation\n",
        "## Phase 5: Comprehensive Testing and Performance Validation\n",
        "\n",
        "This notebook provides comprehensive testing and validation for the AmbuRoute system, including accuracy testing, performance benchmarking, and system validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ§ª Testing & Validation Environment Ready!\n",
            "ðŸ”§ PyTorch version: 2.8.0+cpu\n",
            "ðŸŽ® CUDA available: False\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import time\n",
        "import json\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import YOLOv5 and other ML libraries\n",
        "from ultralytics import YOLO\n",
        "import torch\n",
        "\n",
        "# Set up plotting\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"ðŸ§ª Testing & Validation Environment Ready!\")\n",
        "print(f\"ðŸ”§ PyTorch version: {torch.__version__}\")\n",
        "print(f\"ðŸŽ® CUDA available: {torch.cuda.is_available()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ§ª Comprehensive Testing Framework\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… AmbuRoute Tester ready!\n"
          ]
        }
      ],
      "source": [
        "class AmbuRouteTester:\n",
        "    \"\"\"Comprehensive testing framework for AmbuRoute system\"\"\"\n",
        "    \n",
        "    def __init__(self, model_path, test_data_path=\"dataset/images/test\"):\n",
        "        self.model_path = model_path\n",
        "        self.test_data_path = Path(test_data_path)\n",
        "        self.model = YOLO(model_path)\n",
        "        \n",
        "        # Test results storage\n",
        "        self.test_results = {\n",
        "            'detection_metrics': {},\n",
        "            'performance_metrics': {},\n",
        "            'accuracy_metrics': {},\n",
        "            'system_metrics': {}\n",
        "        }\n",
        "        \n",
        "        print(f\"âœ… AmbuRoute Tester initialized with model: {model_path}\")\n",
        "    \n",
        "    def test_detection_accuracy(self, confidence_threshold=0.5, iou_threshold=0.6):\n",
        "        \"\"\"Test detection accuracy on test dataset\"\"\"\n",
        "        print(\"ðŸ” Testing detection accuracy...\")\n",
        "        \n",
        "        # Get test images\n",
        "        test_images = list(self.test_data_path.glob(\"*.jpg\")) + list(self.test_data_path.glob(\"*.png\"))\n",
        "        \n",
        "        if not test_images:\n",
        "            print(\"âŒ No test images found!\")\n",
        "            return None\n",
        "        \n",
        "        print(f\"ðŸ“Š Testing on {len(test_images)} images\")\n",
        "        \n",
        "        # Initialize metrics\n",
        "        true_positives = 0\n",
        "        false_positives = 0\n",
        "        false_negatives = 0\n",
        "        true_negatives = 0\n",
        "        \n",
        "        detection_results = []\n",
        "        \n",
        "        for img_path in test_images:\n",
        "            # Load image\n",
        "            image = cv2.imread(str(img_path))\n",
        "            if image is None:\n",
        "                continue\n",
        "            \n",
        "            # Run detection\n",
        "            results = self.model(image, conf=confidence_threshold, iou=iou_threshold)\n",
        "            \n",
        "            # Check for ground truth (simplified - assumes filename contains 'ambulance' if positive)\n",
        "            has_ambulance_gt = 'ambulance' in img_path.stem.lower()\n",
        "            \n",
        "            # Check for detections\n",
        "            detected_ambulance = False\n",
        "            detections = []\n",
        "            \n",
        "            for result in results:\n",
        "                if result.boxes is not None:\n",
        "                    for box in result.boxes:\n",
        "                        class_id = int(box.cls[0])\n",
        "                        confidence = float(box.conf[0])\n",
        "                        \n",
        "                        if class_id == 0:  # Ambulance class\n",
        "                            detected_ambulance = True\n",
        "                            detections.append({\n",
        "                                'confidence': confidence,\n",
        "                                'bbox': box.xyxy[0].cpu().numpy()\n",
        "                            })\n",
        "            \n",
        "            # Calculate metrics\n",
        "            if has_ambulance_gt and detected_ambulance:\n",
        "                true_positives += 1\n",
        "            elif not has_ambulance_gt and detected_ambulance:\n",
        "                false_positives += 1\n",
        "            elif has_ambulance_gt and not detected_ambulance:\n",
        "                false_negatives += 1\n",
        "            else:\n",
        "                true_negatives += 1\n",
        "            \n",
        "            # Store results\n",
        "            detection_results.append({\n",
        "                'image_path': str(img_path),\n",
        "                'ground_truth': has_ambulance_gt,\n",
        "                'detected': detected_ambulance,\n",
        "                'detections': detections\n",
        "            })\n",
        "        \n",
        "        # Calculate metrics\n",
        "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
        "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "        accuracy = (true_positives + true_negatives) / (true_positives + false_positives + false_negatives + true_negatives)\n",
        "        \n",
        "        # Store results\n",
        "        self.test_results['detection_metrics'] = {\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1,\n",
        "            'accuracy': accuracy,\n",
        "            'true_positives': true_positives,\n",
        "            'false_positives': false_positives,\n",
        "            'false_negatives': false_negatives,\n",
        "            'true_negatives': true_negatives,\n",
        "            'total_images': len(test_images)\n",
        "        }\n",
        "        \n",
        "        print(f\"âœ… Detection accuracy testing completed!\")\n",
        "        print(f\"   Precision: {precision:.3f}\")\n",
        "        print(f\"   Recall: {recall:.3f}\")\n",
        "        print(f\"   F1-Score: {f1:.3f}\")\n",
        "        print(f\"   Accuracy: {accuracy:.3f}\")\n",
        "        \n",
        "        return self.test_results['detection_metrics']\n",
        "    \n",
        "    def benchmark_performance(self, num_iterations=100, image_size=(640, 640)):\n",
        "        \"\"\"Benchmark model performance\"\"\"\n",
        "        print(\"âš¡ Benchmarking model performance...\")\n",
        "        \n",
        "        # Create dummy image\n",
        "        dummy_image = np.random.randint(0, 255, (*image_size, 3), dtype=np.uint8)\n",
        "        \n",
        "        # Warm up\n",
        "        print(\"ðŸ”¥ Warming up model...\")\n",
        "        for _ in range(10):\n",
        "            _ = self.model(dummy_image)\n",
        "        \n",
        "        # Benchmark inference time\n",
        "        print(f\"ðŸ“Š Running {num_iterations} inference iterations...\")\n",
        "        inference_times = []\n",
        "        \n",
        "        for i in range(num_iterations):\n",
        "            start_time = time.time()\n",
        "            _ = self.model(dummy_image)\n",
        "            end_time = time.time()\n",
        "            inference_times.append(end_time - start_time)\n",
        "        \n",
        "        # Calculate metrics\n",
        "        avg_inference_time = np.mean(inference_times)\n",
        "        std_inference_time = np.std(inference_times)\n",
        "        min_inference_time = np.min(inference_times)\n",
        "        max_inference_time = np.max(inference_times)\n",
        "        fps = 1 / avg_inference_time\n",
        "        \n",
        "        # Memory usage (if available)\n",
        "        memory_usage = 0\n",
        "        if torch.cuda.is_available():\n",
        "            memory_usage = torch.cuda.memory_allocated() / 1024**3  # GB\n",
        "        \n",
        "        # Store results\n",
        "        self.test_results['performance_metrics'] = {\n",
        "            'avg_inference_time': avg_inference_time,\n",
        "            'std_inference_time': std_inference_time,\n",
        "            'min_inference_time': min_inference_time,\n",
        "            'max_inference_time': max_inference_time,\n",
        "            'fps': fps,\n",
        "            'memory_usage_gb': memory_usage,\n",
        "            'iterations': num_iterations\n",
        "        }\n",
        "        \n",
        "        print(f\"âœ… Performance benchmarking completed!\")\n",
        "        print(f\"   Average inference time: {avg_inference_time*1000:.2f} ms\")\n",
        "        print(f\"   FPS: {fps:.2f}\")\n",
        "        print(f\"   Memory usage: {memory_usage:.2f} GB\")\n",
        "        \n",
        "        return self.test_results['performance_metrics']\n",
        "    \n",
        "    def test_confidence_thresholds(self, thresholds=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]):\n",
        "        \"\"\"Test model performance across different confidence thresholds\"\"\"\n",
        "        print(\"ðŸŽ¯ Testing confidence thresholds...\")\n",
        "        \n",
        "        threshold_results = []\n",
        "        \n",
        "        for threshold in thresholds:\n",
        "            print(f\"   Testing threshold: {threshold}\")\n",
        "            \n",
        "            # Get test images\n",
        "            test_images = list(self.test_data_path.glob(\"*.jpg\")) + list(self.test_data_path.glob(\"*.png\"))\n",
        "            \n",
        "            if not test_images:\n",
        "                continue\n",
        "            \n",
        "            # Test with current threshold\n",
        "            true_positives = 0\n",
        "            false_positives = 0\n",
        "            false_negatives = 0\n",
        "            \n",
        "            for img_path in test_images[:50]:  # Limit for speed\n",
        "                # Load image\n",
        "                image = cv2.imread(str(img_path))\n",
        "                if image is None:\n",
        "                    continue\n",
        "                \n",
        "                # Run detection\n",
        "                results = self.model(image, conf=threshold)\n",
        "                \n",
        "                # Check ground truth\n",
        "                has_ambulance_gt = 'ambulance' in img_path.stem.lower()\n",
        "                \n",
        "                # Check detections\n",
        "                detected_ambulance = False\n",
        "                for result in results:\n",
        "                    if result.boxes is not None:\n",
        "                        for box in result.boxes:\n",
        "                            class_id = int(box.cls[0])\n",
        "                            if class_id == 0:  # Ambulance class\n",
        "                                detected_ambulance = True\n",
        "                                break\n",
        "                \n",
        "                # Update metrics\n",
        "                if has_ambulance_gt and detected_ambulance:\n",
        "                    true_positives += 1\n",
        "                elif not has_ambulance_gt and detected_ambulance:\n",
        "                    false_positives += 1\n",
        "                elif has_ambulance_gt and not detected_ambulance:\n",
        "                    false_negatives += 1\n",
        "            \n",
        "            # Calculate metrics\n",
        "            precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
        "            recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
        "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "            \n",
        "            threshold_results.append({\n",
        "                'threshold': threshold,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1_score': f1,\n",
        "                'true_positives': true_positives,\n",
        "                'false_positives': false_positives,\n",
        "                'false_negatives': false_negatives\n",
        "            })\n",
        "        \n",
        "        # Store results\n",
        "        self.test_results['threshold_analysis'] = threshold_results\n",
        "        \n",
        "        print(f\"âœ… Confidence threshold testing completed!\")\n",
        "        \n",
        "        return threshold_results\n",
        "    \n",
        "    def generate_test_report(self):\n",
        "        \"\"\"Generate comprehensive test report\"\"\"\n",
        "        print(\"ðŸ“‹ Generating test report...\")\n",
        "        \n",
        "        report = {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'model_path': str(self.model_path),\n",
        "            'test_results': self.test_results\n",
        "        }\n",
        "        \n",
        "        # Save report\n",
        "        report_path = Path(\"results\") / \"test_report.json\"\n",
        "        report_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        with open(report_path, 'w') as f:\n",
        "            json.dump(report, f, indent=2)\n",
        "        \n",
        "        print(f\"âœ… Test report saved to: {report_path}\")\n",
        "        \n",
        "        return report\n",
        "    \n",
        "    def plot_test_results(self):\n",
        "        \"\"\"Plot comprehensive test results\"\"\"\n",
        "        print(\"ðŸ“Š Plotting test results...\")\n",
        "        \n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "        fig.suptitle('AmbuRoute Test Results', fontsize=16, fontweight='bold')\n",
        "        \n",
        "        # Plot 1: Detection Metrics\n",
        "        if 'detection_metrics' in self.test_results:\n",
        "            metrics = ['Precision', 'Recall', 'F1-Score', 'Accuracy']\n",
        "            values = [\n",
        "                self.test_results['detection_metrics']['precision'],\n",
        "                self.test_results['detection_metrics']['recall'],\n",
        "                self.test_results['detection_metrics']['f1_score'],\n",
        "                self.test_results['detection_metrics']['accuracy']\n",
        "            ]\n",
        "            \n",
        "            bars = axes[0,0].bar(metrics, values, color=['blue', 'red', 'green', 'orange'])\n",
        "            axes[0,0].set_title('Detection Metrics')\n",
        "            axes[0,0].set_ylabel('Score')\n",
        "            axes[0,0].set_ylim(0, 1)\n",
        "            \n",
        "            # Add value labels\n",
        "            for bar, value in zip(bars, values):\n",
        "                height = bar.get_height()\n",
        "                axes[0,0].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                              f'{value:.3f}', ha='center', va='bottom')\n",
        "        \n",
        "        # Plot 2: Performance Metrics\n",
        "        if 'performance_metrics' in self.test_results:\n",
        "            perf_metrics = ['Avg Inference (ms)', 'FPS', 'Memory (GB)']\n",
        "            perf_values = [\n",
        "                self.test_results['performance_metrics']['avg_inference_time'] * 1000,\n",
        "                self.test_results['performance_metrics']['fps'],\n",
        "                self.test_results['performance_metrics']['memory_usage_gb']\n",
        "            ]\n",
        "            \n",
        "            bars = axes[0,1].bar(perf_metrics, perf_values, color=['purple', 'cyan', 'yellow'])\n",
        "            axes[0,1].set_title('Performance Metrics')\n",
        "            axes[0,1].set_ylabel('Value')\n",
        "            \n",
        "            # Add value labels\n",
        "            for bar, value in zip(bars, perf_values):\n",
        "                height = bar.get_height()\n",
        "                axes[0,1].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                              f'{value:.2f}', ha='center', va='bottom')\n",
        "        \n",
        "        # Plot 3: Confusion Matrix\n",
        "        if 'detection_metrics' in self.test_results:\n",
        "            cm_data = [\n",
        "                [self.test_results['detection_metrics']['true_positives'], \n",
        "                 self.test_results['detection_metrics']['false_positives']],\n",
        "                [self.test_results['detection_metrics']['false_negatives'], \n",
        "                 self.test_results['detection_metrics']['true_negatives']]\n",
        "            ]\n",
        "            \n",
        "            sns.heatmap(cm_data, annot=True, fmt='d', ax=axes[0,2], \n",
        "                       xticklabels=['Predicted Positive', 'Predicted Negative'],\n",
        "                       yticklabels=['Actual Positive', 'Actual Negative'])\n",
        "            axes[0,2].set_title('Confusion Matrix')\n",
        "        \n",
        "        # Plot 4: Confidence Threshold Analysis\n",
        "        if 'threshold_analysis' in self.test_results:\n",
        "            thresholds = [r['threshold'] for r in self.test_results['threshold_analysis']]\n",
        "            precision_vals = [r['precision'] for r in self.test_results['threshold_analysis']]\n",
        "            recall_vals = [r['recall'] for r in self.test_results['threshold_analysis']]\n",
        "            f1_vals = [r['f1_score'] for r in self.test_results['threshold_analysis']]\n",
        "            \n",
        "            axes[1,0].plot(thresholds, precision_vals, 'o-', label='Precision', color='blue')\n",
        "            axes[1,0].plot(thresholds, recall_vals, 'o-', label='Recall', color='red')\n",
        "            axes[1,0].plot(thresholds, f1_vals, 'o-', label='F1-Score', color='green')\n",
        "            axes[1,0].set_title('Confidence Threshold Analysis')\n",
        "            axes[1,0].set_xlabel('Confidence Threshold')\n",
        "            axes[1,0].set_ylabel('Score')\n",
        "            axes[1,0].legend()\n",
        "            axes[1,0].grid(True)\n",
        "        \n",
        "        # Plot 5: Performance Distribution\n",
        "        if 'performance_metrics' in self.test_results:\n",
        "            # Simulate inference time distribution\n",
        "            mean_time = self.test_results['performance_metrics']['avg_inference_time']\n",
        "            std_time = self.test_results['performance_metrics']['std_inference_time']\n",
        "            times = np.random.normal(mean_time, std_time, 1000)\n",
        "            \n",
        "            axes[1,1].hist(times * 1000, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "            axes[1,1].axvline(mean_time * 1000, color='red', linestyle='--', label=f'Mean: {mean_time*1000:.2f}ms')\n",
        "            axes[1,1].set_title('Inference Time Distribution')\n",
        "            axes[1,1].set_xlabel('Inference Time (ms)')\n",
        "            axes[1,1].set_ylabel('Frequency')\n",
        "            axes[1,1].legend()\n",
        "        \n",
        "        # Plot 6: System Summary\n",
        "        axes[1,2].text(0.1, 0.8, 'AmbuRoute Test Summary', fontsize=14, fontweight='bold')\n",
        "        \n",
        "        if 'detection_metrics' in self.test_results:\n",
        "            dm = self.test_results['detection_metrics']\n",
        "            summary_text = f\"\"\"\n",
        "Precision: {dm['precision']:.3f}\n",
        "Recall: {dm['recall']:.3f}\n",
        "F1-Score: {dm['f1_score']:.3f}\n",
        "Accuracy: {dm['accuracy']:.3f}\n",
        "\n",
        "Total Images: {dm['total_images']}\n",
        "True Positives: {dm['true_positives']}\n",
        "False Positives: {dm['false_positives']}\n",
        "False Negatives: {dm['false_negatives']}\n",
        "            \"\"\"\n",
        "            axes[1,2].text(0.1, 0.6, summary_text, fontsize=10, verticalalignment='top')\n",
        "        \n",
        "        axes[1,2].set_xlim(0, 1)\n",
        "        axes[1,2].set_ylim(0, 1)\n",
        "        axes[1,2].axis('off')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        print(\"âœ… Test results plotted successfully!\")\n",
        "\n",
        "# Initialize tester\n",
        "print(\"âœ… AmbuRoute Tester ready!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
